{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Предобработка текста и определение языка</h1></center>\n",
    "\n",
    "\n",
    "### Вариант А\n",
    "\n",
    "Написать классификатор для автоматического определения языка текста.\n",
    "Данные:\n",
    "Выбрать 4 языка с похожими алфавитами.\n",
    "Набрать обуч. выборку — (хотя бы) по 100 статей из википедии на каждый язык. Бонус: скачать дампы википедии для выбранных языков, почистить и использовать в качестве обучающей выборки их.\n",
    "Проверочная выборка: скачать (хотя бы) по 30 статей из википедии на каждый язык.\n",
    "\n",
    "Признаки для обучения (векторизация документов): символьные триграммы.\n",
    "\n",
    "Предобработка:\n",
    "Удалить из всех текстов символы/токены/части документов, которые добавят шума на этапе векторизации или затруднят классификацию.\n",
    "Выбрать по 3 статьи для каждого языка и подробно описать свои решения по препроцессингу.\n",
    "\n",
    "Векторизовав тексты, построить матрицу схожести текстов, визуализировать её.\n",
    "\n",
    "Какие языки оказались очень похожи друг на друга, а какие нет? Иными словами, какие языки проще различить по нграммам, а какие труднее? Выбрать один язык. С какими языками его легко спутать (по вашей векторизации)?\n",
    "\n",
    "Обучить классификатор (например, NaiveBayes или SVM).\n",
    "Оценить качество работы системы: посчитать точность, полноту, f-меру.\n",
    "\n",
    "Сколько обучающих данных достаточно для получения хороших результатов? Бонус: график зависимости accuracy от объёма обучающей выборки.\n",
    "Сравните качество класификации на униграммах, биграммах, 3-граммах, 4-граммах, 5-граммах. Бонус: постройте такой график. \n",
    "Какие именно нграммы оказались наиболее полезными признаками? (например, содержащие символы, которые используются лишь в одно алфавите?)\n",
    "Интерпретировать результаты.\n",
    "\n",
    "Оформление: в свой репозиторий загрузить папку langdetect_a_hw, в которой должны быть:\n",
    "lang_detect_results.txt (или .md или .pdf) с описанием всех ваших шагов (сколько каких текстов взяли, какие методы использовали, какие результаты получились). Описание шага предобработки — обязательно;\n",
    "проверочная выборка, на которой вы тестировали классификатор (или, что предпочтительнее, файл с ссылками на статьи википедии, которые вы использовали);\n",
    "код в .ipynb либо .py;\n",
    "\n",
    "\n",
    "### Вариант B\n",
    "\n",
    "Данные: всеобщая декларация прав человека (см. то, что делали на занятии).\n",
    "Бонус: выкачать по 100 статей из википедии для 7 языков и использовать их вместо маленького датасета.\n",
    "\n",
    "Предобработка: \n",
    "Удалить из всех текстов символы/токены/части документов, которые затруднят классификацию.\n",
    "Выбрать по 3 текста для каждого языка и подробно описать свои решения по препроцессингу.\n",
    "\n",
    "Собрать для каждого языка частотный список слов/биграмм/триграмм. Записать топ-300 в текстовый файл (по одному нграмму на строке, от более частого к менее частому).\n",
    "Построить матрицу схожести языков, визуализировать её. Схожесть считаем как объём пересечения полученных топов нграмм (сколько нграммов попали в оба топа?).\n",
    "Какие языки оказались очень похожи друг на друга, а какие нет? Иными словами, какие языки проще различить по нграммам, а какие труднее? Интерпретируйте результат.\n",
    "Оформление: в свой репозиторий загрузить папку langdetect_b_hw, в которой должны быть:\n",
    "описание шага предобработки — обязательно;\n",
    "7 файлов с наиболее частотными нграммами (например, ru.txt, uk.txt)\n",
    "визуализация матрицы схожести (например, similarity_matrix.png)\n",
    "интерпретация результатов (lang_detect_results.txt или .md или .pdf)\n",
    "код в .ipynb либо .py\n",
    "\n",
    "Бонус: в википедии есть статья о Декларации (на разных языках). Скопируйте тексты этих статей в txt файлы (например, decl_ru.txt) и определите автоматически язык статьи: посчитайте топ слов/нграмм, сравните объём пересечения этого топа с топами для разных языков, считайте за ответ наиболее похожий. Оцените качество такой системы. Как изменится качество распознавания языка, если проверять не на всей статье, а лишь на одном предложении/абзаце?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from pickle import dump, load\n",
    "from itertools import chain\n",
    "from pandas import DataFrame\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['en', 'fr', 'cz', 'es']\n",
    "pages, labels = defaultdict(lambda: []), defaultdict(lambda: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    reg_exps = (re.compile('[^\\w ]+'), re.compile('[\\d]+'), re.compile('[ ]+'))\n",
    "    for reg in reg_exps:\n",
    "        text = reg.sub(' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language_id, language in enumerate(languages):\n",
    "    wikipedia.set_lang(language)\n",
    "    for page_title in wikipedia.random(10):\n",
    "        title = page_title\n",
    "        if 'disambig' in page_title: \n",
    "            title = page_title.split(' ')[0] \n",
    "        while(True):\n",
    "            try:\n",
    "                normalized_text = normalize(wikipedia.page(str(wikipedia.search(title)[0])).content)\n",
    "                pages[language].append(normalized_text)\n",
    "                labels[language].append(language_id)\n",
    "                break\n",
    "            except wikipedia.DisambiguationError:\n",
    "                title = wikipedia.random(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(list(zip(list(chain.from_iterable(pages[language] for language in languages)), \n",
    "                          list(chain.from_iterable(labels[language] for language in languages)))),\n",
    "                          columns=['text', 'language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('wikipedia_languages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame.from_csv('wikipedia_languages.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectPercentile(percentile=100)\n",
    "clf = LogisticRegression(tol=1e-8, penalty='l2', C=1)\n",
    "svm1 = svm.LinearSVC()\n",
    "svm2 = svm.SVC()\n",
    "countvect_char_wb =[CountVectorizer(ngram_range=(3, 3), analyzer='char_wb'), TfidfVectorizer(ngram_range=(1, 3), analyzer='char', binary=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.583333333333\n"
     ]
    }
   ],
   "source": [
    "for countvect in countvect_char_wb:\n",
    "    char_model = Pipeline([('vect', countvect), ('select', select), ('logr', clf)])\n",
    "    char_model.fit(train.text.values, train.language.values)\n",
    "    print(f1_score(char_model.predict(test.text.values), test.language.values, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
