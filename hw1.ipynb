{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Предобработка текста и определение языка</h1></center>\n",
    "\n",
    "\n",
    "### Вариант А\n",
    "\n",
    "Написать классификатор для автоматического определения языка текста.\n",
    "Данные:\n",
    "Выбрать 4 языка с похожими алфавитами.\n",
    "Набрать обуч. выборку — (хотя бы) по 100 статей из википедии на каждый язык. Бонус: скачать дампы википедии для выбранных языков, почистить и использовать в качестве обучающей выборки их.\n",
    "Проверочная выборка: скачать (хотя бы) по 30 статей из википедии на каждый язык.\n",
    "\n",
    "Признаки для обучения (векторизация документов): символьные триграммы.\n",
    "\n",
    "Предобработка:\n",
    "Удалить из всех текстов символы/токены/части документов, которые добавят шума на этапе векторизации или затруднят классификацию.\n",
    "Выбрать по 3 статьи для каждого языка и подробно описать свои решения по препроцессингу.\n",
    "\n",
    "Векторизовав тексты, построить матрицу схожести текстов, визуализировать её.\n",
    "\n",
    "Какие языки оказались очень похожи друг на друга, а какие нет? Иными словами, какие языки проще различить по нграммам, а какие труднее? Выбрать один язык. С какими языками его легко спутать (по вашей векторизации)?\n",
    "\n",
    "Обучить классификатор (например, NaiveBayes или SVM).\n",
    "Оценить качество работы системы: посчитать точность, полноту, f-меру.\n",
    "\n",
    "Сколько обучающих данных достаточно для получения хороших результатов? Бонус: график зависимости accuracy от объёма обучающей выборки.\n",
    "Сравните качество класификации на униграммах, биграммах, 3-граммах, 4-граммах, 5-граммах. Бонус: постройте такой график. \n",
    "Какие именно нграммы оказались наиболее полезными признаками? (например, содержащие символы, которые используются лишь в одно алфавите?)\n",
    "Интерпретировать результаты.\n",
    "\n",
    "Оформление: в свой репозиторий загрузить папку langdetect_a_hw, в которой должны быть:\n",
    "lang_detect_results.txt (или .md или .pdf) с описанием всех ваших шагов (сколько каких текстов взяли, какие методы использовали, какие результаты получились). Описание шага предобработки — обязательно;\n",
    "проверочная выборка, на которой вы тестировали классификатор (или, что предпочтительнее, файл с ссылками на статьи википедии, которые вы использовали);\n",
    "код в .ipynb либо .py;\n",
    "\n",
    "\n",
    "### Вариант B\n",
    "\n",
    "Данные: всеобщая декларация прав человека (см. то, что делали на занятии).\n",
    "Бонус: выкачать по 100 статей из википедии для 7 языков и использовать их вместо маленького датасета.\n",
    "\n",
    "Предобработка: \n",
    "Удалить из всех текстов символы/токены/части документов, которые затруднят классификацию.\n",
    "Выбрать по 3 текста для каждого языка и подробно описать свои решения по препроцессингу.\n",
    "\n",
    "Собрать для каждого языка частотный список слов/биграмм/триграмм. Записать топ-300 в текстовый файл (по одному нграмму на строке, от более частого к менее частому).\n",
    "Построить матрицу схожести языков, визуализировать её. Схожесть считаем как объём пересечения полученных топов нграмм (сколько нграммов попали в оба топа?).\n",
    "Какие языки оказались очень похожи друг на друга, а какие нет? Иными словами, какие языки проще различить по нграммам, а какие труднее? Интерпретируйте результат.\n",
    "Оформление: в свой репозиторий загрузить папку langdetect_b_hw, в которой должны быть:\n",
    "описание шага предобработки — обязательно;\n",
    "7 файлов с наиболее частотными нграммами (например, ru.txt, uk.txt)\n",
    "визуализация матрицы схожести (например, similarity_matrix.png)\n",
    "интерпретация результатов (lang_detect_results.txt или .md или .pdf)\n",
    "код в .ipynb либо .py\n",
    "\n",
    "Бонус: в википедии есть статья о Декларации (на разных языках). Скопируйте тексты этих статей в txt файлы (например, decl_ru.txt) и определите автоматически язык статьи: посчитайте топ слов/нграмм, сравните объём пересечения этого топа с топами для разных языков, считайте за ответ наиболее похожий. Оцените качество такой системы. Как изменится качество распознавания языка, если проверять не на всей статье, а лишь на одном предложении/абзаце?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['en', 'fr', 'ru', 'es']\n",
    "pages = defaultdict(lambda: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    wikipedia.set_lang(language)\n",
    "    for page_title in wikipedia.random(150):\n",
    "        title = page_title\n",
    "        if 'disambig' in page_title: \n",
    "            title = page_title.split(' ')[0] \n",
    "        try:\n",
    "            pages[language].append(wikipedia.page(str(wikipedia.search(title)[0])).content)\n",
    "        except wikipedia.DisambiguationError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pages.pickle', 'wb') as f:\n",
    "    dump(dict(pages), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим 100 страниц для обучения для каждого языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chunk = 100\n",
    "\n",
    "for language in languages:\n",
    "    pages[language] = pages[language][:train_chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pages_train.pickle', 'wb') as f:\n",
    "    dump(dict(pages), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
