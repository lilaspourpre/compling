{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Предобработка текста и определение языка (Вариант А)</h1></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Данное исследование посвящено проблеме автоматического определения языка в близкородственных языках (т.е. языках с одинаковым алфавитом). Задача состоит в том, чтобы определить, к какому из известного набор n языков относится рассматриваемый документ.\n",
    "\n",
    "Для решения этой задачи мы разбиваем слова каждого языка на символьные триграмы -- все возможные комбинации подряд идущих троек символов -- таким образом, для каждого языка набор таких триграмов будет уникален, и большое число совпадений триграмов в документе с одним из наборов и будет говорить о языке документа.\n",
    "\n",
    "Теперь вопрос возникает в том, откуда взять эти наборы для каждого языка, и каким образом подсчитывать совпадения. Мы считатем, что данная задача может быть решена при помощи алгоритмов машинного обучения и Википедии. Иными словами, для получения триграмов будет выкачано по *n* случайных статей Википедии для каждого из k языков. Далее, все n\\*k статей будет объединены в набор, каждая статья в котором будет векторизована по документному индексу n-грамов: каждой статье будет ассиироцан вектор, компонентами которого будут являться ВСЕ возможные n-грамы, встречающиеся в наборе (т.е. на всех языках), а значениями будут являтья число вхождений данного n-грама в документ. Таким образом, предполагается, что вектора документов на одном и том же языки будут похожи, потому что в них будут относительно часто встречаться некоторые часто используемые n-грамы на одном языке, а на других языках встречаться не будут.\n",
    "\n",
    "При этом мы также знаем язык каждого документа из рассматриваемой выборки. Значит, данную задачу мы можем рассматривать как задачу классификации, и использовать в качестве признакового пространство полученные документные вектора, а в качестве меток классов -- языки документов. Теоретически, для каждого нового документа мы сможем таким образом предскзаывать язык, на котором он написан -- т.е. метку класса в терминах классификации. Для того, чтобы проверить, насколько хорошо работает предлагаемый нами алгоритм, мы будем использовать кросс-валидацию на обучающей выборке документов, используя её часть как тестовый сет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from pickle import dump, load\n",
    "from itertools import chain\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы решили использовать следующий набор языков: английский, французский, испанский, чешский. Таким образом, задача будет заключаться в том, чтобы определить, на каком языке из этого набора написан рассматриваемый документ. \n",
    "\n",
    "\n",
    "Предобработка:\n",
    "Удалить из всех текстов символы/токены/части документов, которые добавят шума на этапе векторизации или затруднят классификацию.\n",
    "Выбрать по 3 статьи для каждого языка и подробно описать свои решения по препроцессингу.\n",
    "\n",
    "Векторизовав тексты, построить матрицу схожести текстов, визуализировать её.\n",
    "\n",
    "Какие языки оказались очень похожи друг на друга, а какие нет? Иными словами, какие языки проще различить по нграммам, а какие труднее? Выбрать один язык. С какими языками его легко спутать (по вашей векторизации)?\n",
    "\n",
    "Обучить классификатор (например, NaiveBayes или SVM).\n",
    "Оценить качество работы системы: посчитать точность, полноту, f-меру.\n",
    "\n",
    "Сколько обучающих данных достаточно для получения хороших результатов? Бонус: график зависимости accuracy от объёма обучающей выборки.\n",
    "Сравните качество класификации на униграммах, биграммах, 3-граммах, 4-граммах, 5-граммах. Бонус: постройте такой график. \n",
    "Какие именно нграммы оказались наиболее полезными признаками? (например, содержащие символы, которые используются лишь в одно алфавите?)\n",
    "Интерпретировать результаты.\n",
    "\n",
    "Оформление: в свой репозиторий загрузить папку langdetect_a_hw, в которой должны быть:\n",
    "lang_detect_results.txt (или .md или .pdf) с описанием всех ваших шагов (сколько каких текстов взяли, какие методы использовали, какие результаты получились). Описание шага предобработки — обязательно;\n",
    "проверочная выборка, на которой вы тестировали классификатор (или, что предпочтительнее, файл с ссылками на статьи википедии, которые вы использовали);\n",
    "код в .ipynb либо .py;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = ['en', 'fr', 'cz', 'es']\n",
    "pages, labels = defaultdict(lambda: []), defaultdict(lambda: [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень важный этап при работе с текстом это предобработка. Предобработка заключается в удалении элементов текста, которые могут добавить шум -- лишнюю информацию, затрудняющую и ухудшаю работку классификатора. В нашем случае шумом будут являться все небуквенные символы (non-alphanumeric), гиперссылки, и тому подобное. Для предобработки были выбраны регулярные выражения, ощичающие текст до формата: \"слова без чисел, знаков препинания, знаков табуляции, разделенные пробелами\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    reg_exps = (re.compile('[^\\w ]+'), re.compile('[\\d]+'), re.compile('[ ]+'))\n",
    "    for reg in reg_exps:\n",
    "        text = reg.sub(' ', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(languages[0])\n",
    "article = wikipedia.page(wikipedia.random()).content\n",
    "print(article[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(article[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как мы выбрали методы предобработки, перейдём непосредственно к краулингу статей из Википедии. Мы решили взять 1200 статей, для каждого из языков будет по 300 :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AMOUNT_OF_PAGES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language_id, language in enumerate(languages):\n",
    "    wikipedia.set_lang(language)\n",
    "    for page_title in wikipedia.random(AMOUNT_OF_PAGES):\n",
    "        title = page_title\n",
    "        if 'disambig' in page_title: \n",
    "            title = page_title.split(' ')[0] \n",
    "        while(True):\n",
    "            try:\n",
    "                normalized_text = normalize(wikipedia.page(str(wikipedia.search(title)[0])).content)\n",
    "                pages[language].append(normalized_text)\n",
    "                labels[language].append(language_id)\n",
    "                break\n",
    "            except wikipedia.DisambiguationError:\n",
    "                title = wikipedia.random(1)\n",
    "            except wikipedia.PageError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Для того, чтобы работать со статьями было удобнее, засунем их в ```DataFrame``` и сериализуем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = 'wikipedia_languages.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(list(zip(list(chain.from_iterable(pages[language] for language in languages)), \n",
    "                          list(chain.from_iterable(labels[language] for language in languages)))),\n",
    "                          columns=['text', 'language'])\n",
    "\n",
    "data.to_csv(FILENAME, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Теперь скрауленные данные хранятся на диске, и для того, чтобы обращаться к ним, не нужно скаичивать все статьи заново:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataFrame.from_csv('langs.csv', encoding='utf-8').dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти данные мы можем преобразовать в признаковые вектора с помощью различных векторизаторов; целевым вектором соответственно будет выступать с метками языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvect_char_wb = [('2-gram', TfidfVectorizer(ngram_range=(1, 2), analyzer='char', binary=False)), \n",
    "                    ('3-gram',TfidfVectorizer(ngram_range=(1, 3), analyzer='char', binary=False)),\n",
    "                    ('4-gram',TfidfVectorizer(ngram_range=(1, 4), analyzer='char', binary=False)),\n",
    "                    ('5-gram',TfidfVectorizer(ngram_range=(1, 5), analyzer='char', binary=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(svm.SVC(kernel='poly',C=0.1), 'Linear SVC', 'o', 'brown'), # add more classifiers here\n",
    "              (LogisticRegression(tol=1e-8, penalty='l2', C=0.1), 'Logistic Regression', 'v', 'green'),\n",
    "             (RandomForestClassifier(n_estimators=50), 'Random Forest', 'v', 'blue')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.language.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, с помощью кросс-валидации мы можем оценить качество классификации по $F_1$-мере, не используя отдельную тестовую выборку. Мы можем проверить качество решения задачи на разных классификаторах и проверить его зависимость от объёма тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CROSS_VAL_FOLDS = 3\n",
    "TEST_CHUNK = 0.4\n",
    "CROSS_VAL_CHUNK = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for countvect in countvect_char_wb:\n",
    "    X = countvect[1].fit_transform(data.text.values)\n",
    "    for estimator, name, markerstyle, colorstyle in estimators:\n",
    "        cv = ShuffleSplit(n_splits=CROSS_VAL_FOLDS, test_size=TEST_CHUNK, random_state=0)\n",
    "        train_sizes=np.linspace(TEST_CHUNK, CROSS_VAL_CHUNK, CROSS_VAL_FOLDS)\n",
    "        train_sizes, train_scores, test_scores = learning_curve(estimator, X, Y, cv=cv, train_sizes=train_sizes)\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        plt.plot(train_sizes, train_scores_mean, markersize=15, label=name, linewidth=3, color=colorstyle)\n",
    "    plt.title('Results on {}'.format(countvect[0]), fontsize=20)\n",
    "    plt.grid(True, axis='y', linewidth=1, color='black')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1,1), prop={'size':20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построив confusion matrix, мы можем определить, какие языки оказались очень похожи друг на друга, а какие нет (иными словами, какие языки проще различить по n-грамам)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOWER_BOUND = 1\n",
    "UPPER_BOUND = 25\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for countvect in countvect_char_wb:\n",
    "    sp_matrix = countvect[1].fit_transform(train.text.values[LOWER_BOUND:UPPER_BOUND], train.language.values[LOWER_BOUND:UPPER_BOUND])\n",
    "    similarities = cosine_similarity(sp_matrix)\n",
    "    lan = [languages[i] for i in train.language.values[LOWER_BOUND:UPPER_BOUND]]\n",
    "    df_cm = DataFrame(similarities, columns=lan, index=lan)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title('Similarity for texts from '+countvect[0], fontsize=16)\n",
    "    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for countvect in countvect_char_wb:\n",
    "    for estimator, name, markerstyle, colorstyle in estimators:\n",
    "        char_model = Pipeline([('vect', countvect[1]), ('logr', estimator)])\n",
    "        char_model.fit(train.text.values, train.language.values)\n",
    "        target_names = languages\n",
    "        report = classification_report(test.language.values, char_model.predict(test.text.values), target_names=target_names)\n",
    "        print('Vectorizer: {}, Estimator: {}\\n{}\\nMicro F1-score={:0.2f}, Precision={:0.2f}, Recall={:0.2f}'.format(countvect[0], name, report, \n",
    "                                                                        f1_score(char_model.predict(test.text.values), test.language.values, average='micro'),\n",
    "                                                                        precision_score(char_model.predict(test.text.values), test.language.values, average='micro'),\n",
    "                                                                        precision_score(char_model.predict(test.text.values), test.language.values, average='micro')))\n",
    "        print('=====================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Таким образом, в проведенном исследовании мы предложили алгоритм для автоматического определения языка по n-грамам. Мы сравнили несколько разных способов векторизации и несколько разных классификаторов; нам удалось достигнуть хорошего качество, а также понять, какие языки являются более похожими друг на друга, а какие менее.\n",
    "\n",
    "В продолжение данной работы мы планируем попробовать применить глубокие нейронные сети для классификации (например, LSTM), а также исследовать, могут ли другие лингвистические особенности языка предсказывать его -- например, падежная стратегия кодирования или порядок частей речи в предложении."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
